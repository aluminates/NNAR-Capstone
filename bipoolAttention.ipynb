{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNrQzQYQTJbYPmxKVUjZOSZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"KVXvjvE62Baq"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import os\n","from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"id":"BDMpRnZqcDDT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730813275838,"user_tz":-330,"elapsed":31154,"user":{"displayName":"NNAR-CAPSTONE","userId":"01323149097215678909"}},"outputId":"3b8d8a56-1d5d-47de-f3d7-e2771c4a5242"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Defining Model Classes"],"metadata":{"id":"MumMV17C2Sod"}},{"cell_type":"markdown","source":["## Compact Bilinear Pooling"],"metadata":{"id":"dAQktI9U2V5B"}},{"cell_type":"code","source":["class CompactBilinearPooling(nn.Module):\n","    def __init__(self, input_dim1, input_dim2, output_dim, sum_pool=True):\n","        super().__init__()\n","        self.input_dim1 = input_dim1\n","        self.input_dim2 = input_dim2\n","        self.output_dim = output_dim\n","        self.sum_pool = sum_pool\n","\n","        self.sketch1 = nn.Parameter(self._generate_sketch_matrix(input_dim1, output_dim), requires_grad=False)\n","        self.sketch2 = nn.Parameter(self._generate_sketch_matrix(input_dim2, output_dim), requires_grad=False)\n","\n","    def _generate_sketch_matrix(self, input_dim, output_dim):\n","        sketch = torch.zeros(input_dim, output_dim)\n","        for i in range(input_dim):\n","            h = np.random.randint(output_dim)\n","            s = np.random.choice([-1, 1])\n","            sketch[i, h] = s\n","        return sketch\n","\n","    def forward(self, x1, x2):\n","        batch_size = x1.size(0)\n","\n","        # Compute count sketches\n","        sketch1 = torch.mm(x1, self.sketch1)\n","        sketch2 = torch.mm(x2, self.sketch2)\n","\n","        fft1 = torch.fft.rfft(sketch1, dim=1)\n","        fft2 = torch.fft.rfft(sketch2, dim=1)\n","\n","        fft_product = fft1 * fft2\n","\n","        cbp = torch.fft.irfft(fft_product, n=self.output_dim, dim=1)\n","\n","        if self.sum_pool:\n","            cbp = cbp.sum(dim=1)\n","\n","        return cbp"],"metadata":{"id":"GRZKmnS0iXeB","executionInfo":{"status":"ok","timestamp":1730813281472,"user_tz":-330,"elapsed":1019,"user":{"displayName":"NNAR-CAPSTONE","userId":"01323149097215678909"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["This layer is used to combine two different types of feature embeddings (video and text) into a single representation. Compact Bilinear Pooling is efficient in combining these features while capturing rich interactions between them.\n","\n","Sketch Matrices are used to project the input features into a higher-dimensional space. This projection helps in capturing interactions between the video and text features.\n","\n","Fast Fourier Transform is applied to the count sketches of the input features, which helps in efficiently computing the outer product in the projected space."],"metadata":{"id":"WjHcJlanKTdz"}},{"cell_type":"markdown","source":["## Multihead Attention"],"metadata":{"id":"KKeibdV32dJ6"}},{"cell_type":"markdown","source":["Allows the model to focus on different parts of the input sequences. By using multiple heads, the model can learn to attend to various aspects of the input data simultaneously.\n","\n","Linear layers are used to project the input into different spaces for computing query, key, and value.\n","\n","Attention Weights are computed using the dot product of the query and key matrices, followed by a softmax function."],"metadata":{"id":"ErSyzUGFKULn"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads, dropout=0.1):\n","        super().__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","\n","        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n","\n","        self.q_proj = nn.Linear(embed_dim, embed_dim)\n","        self.k_proj = nn.Linear(embed_dim, embed_dim)\n","        self.v_proj = nn.Linear(embed_dim, embed_dim)\n","        self.out_proj = nn.Linear(embed_dim, embed_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None):\n","        batch_size, seq_len, _ = query.size()\n","\n","        q = self.q_proj(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","        k = self.k_proj(key).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","        v = self.v_proj(value).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","\n","        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n","\n","        if attn_mask is not None:\n","            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n","\n","        if key_padding_mask is not None:\n","            scores = scores.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))\n","\n","        attn_weights = F.softmax(scores, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        attn_output = torch.matmul(attn_weights, v)\n","        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n","\n","        return self.out_proj(attn_output)"],"metadata":{"id":"6PxdNnnp2fuB","executionInfo":{"status":"ok","timestamp":1730813286943,"user_tz":-330,"elapsed":1029,"user":{"displayName":"NNAR-CAPSTONE","userId":"01323149097215678909"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Positional Encoding"],"metadata":{"id":"q5j7Dwlh2iDp"}},{"cell_type":"markdown","source":["Since the transformer model doesnâ€™t inherently capture the order of sequences, positional encoding is added to the input embeddings to provide some notion of order or position.\n","\n","Sine and Cosine functions are used to create the positional encodings, which are then added to the input embeddings."],"metadata":{"id":"dQQYyWb_Kv6i"}},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.max_len = max_len\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        seq_len = x.size(1)\n","        return x + self.pe[:, :seq_len]"],"metadata":{"id":"fzJ1lmEN2jz3","executionInfo":{"status":"ok","timestamp":1730813291757,"user_tz":-330,"elapsed":4,"user":{"displayName":"NNAR-CAPSTONE","userId":"01323149097215678909"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Transformer Encoding"],"metadata":{"id":"xfzdm1Fv2sa3"}},{"cell_type":"markdown","source":["The Transformer Encoder processes the input embeddings in parallel using self-attention and feedforward layers, allowing the model to capture complex dependencies in the data."],"metadata":{"id":"ng4bauLvK0lt"}},{"cell_type":"code","source":["class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n","        super().__init__()\n","        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout)\n","        self.linear1 = nn.Linear(d_model, dim_feedforward)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear2 = nn.Linear(dim_feedforward, d_model)\n","\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n","        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n","        src = src + self.dropout1(src2)\n","        src = self.norm1(src)\n","        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n","        src = src + self.dropout2(src2)\n","        src = self.norm2(src)\n","        return src"],"metadata":{"id":"1uR06rgC2uq4","executionInfo":{"status":"ok","timestamp":1730813296977,"user_tz":-330,"elapsed":596,"user":{"displayName":"NNAR-CAPSTONE","userId":"01323149097215678909"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, encoder_layer, num_layers):\n","        super().__init__()\n","        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n","\n","    def forward(self, src, mask=None, src_key_padding_mask=None):\n","        output = src\n","        for layer in self.layers:\n","            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n","        return output"],"metadata":{"id":"HnyytT7v2vfz","executionInfo":{"status":"ok","timestamp":1730813301814,"user_tz":-330,"elapsed":846,"user":{"displayName":"NNAR-CAPSTONE","userId":"01323149097215678909"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Fusion Model"],"metadata":{"id":"4t8I_um12zrA"}},{"cell_type":"code","source":["class MultimodalFusion(nn.Module):\n","    def __init__(self, video_dim, text_dim, fusion_dim, num_heads=8, num_encoder_layers=6):\n","        super().__init__()\n","        self.video_projection = nn.Linear(video_dim, fusion_dim)\n","        self.text_projection = nn.Linear(text_dim, fusion_dim)\n","        self.compact_bilinear = CompactBilinearPooling(fusion_dim, fusion_dim, fusion_dim)\n","\n","        self.pos_encoder = PositionalEncoding(fusion_dim)\n","        encoder_layer = TransformerEncoderLayer(fusion_dim, num_heads)\n","        self.transformer_encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n","\n","        self.final_projection = nn.Linear(fusion_dim, fusion_dim)\n","\n","    def forward(self, video_features, text_features):\n","        video_proj = self.video_projection(video_features)  # [batch_size, fusion_dim]\n","        text_proj = self.text_projection(text_features)     # [batch_size, fusion_dim]\n","\n","        fused_features = self.compact_bilinear(video_proj, text_proj)  # [batch_size, fusion_dim]\n","        fused_features = fused_features.unsqueeze(1)  # [batch_size, 1, fusion_dim]\n","\n","        fused_features = self.pos_encoder(fused_features)  # Apply positional encoding\n","\n","        encoded_features = self.transformer_encoder(fused_features)  # [batch_size, 1, fusion_dim]\n","        encoded_features = encoded_features.squeeze(1)  # [batch_size, fusion_dim]\n","\n","        output = self.final_projection(encoded_features)  # [batch_size, fusion_dim]\n","\n","        return output"],"metadata":{"id":"nBGFlUQP26dP","executionInfo":{"status":"ok","timestamp":1730813305620,"user_tz":-330,"elapsed":636,"user":{"displayName":"NNAR-CAPSTONE","userId":"01323149097215678909"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# File Loading"],"metadata":{"id":"IHODbIEL27nZ"}},{"cell_type":"code","source":["def get_base_name_from_text_filename(text_filename):\n","    \"\"\" Extract base name from text filename before '_embedding.npy' \"\"\"\n","    return text_filename.split('_embedding.npy')[0]\n","\n","def find_matching_text_filename(video_filename, text_filenames):\n","    \"\"\" Find a matching text filename based on base name in video filename. \"\"\"\n","    video_base_name = video_filename.split('_embeddings.npy')[0]\n","    for text_filename in text_filenames:\n","        text_base_name = get_base_name_from_text_filename(text_filename)\n","        if text_base_name in video_base_name:\n","            return text_filename\n","    return None\n","\n","def find_anomaly_type(filename):\n","    if 'AbandonedObject' in filename:\n","        return 'abandonedobject'\n","    elif 'Vandalism' in filename:\n","        return 'vandalism'\n","    elif 'Violence' in filename:\n","        return 'violence'\n","    elif 'Normal' in filename:\n","        return 'normal'\n","    else:\n","        return 'unknown'"],"metadata":{"id":"daxu0RI13EbB","executionInfo":{"status":"ok","timestamp":1730813340293,"user_tz":-330,"elapsed":821,"user":{"displayName":"NNAR-CAPSTONE","userId":"01323149097215678909"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Paths to feature directories\n","video_features_dir = '/content/drive/MyDrive/video_embeddings'\n","text_features_dir = '/content/drive/MyDrive/rowtext_embeddings'"],"metadata":{"id":"J13ZNDhV3KPj","executionInfo":{"status":"ok","timestamp":1730813346553,"user_tz":-330,"elapsed":815,"user":{"displayName":"NNAR-CAPSTONE","userId":"01323149097215678909"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Load video features\n","video_feature_list = []\n","video_filenames = []\n","for file_name in sorted(os.listdir(video_features_dir)):\n","    if file_name.endswith('.npy'):\n","        file_path = os.path.join(video_features_dir, file_name)\n","        video_features = np.load(file_path)\n","        video_feature_list.append(video_features)\n","        video_filenames.append(file_name)\n","\n","# Load text features\n","text_feature_list = []\n","text_filenames = []\n","for file_name in sorted(os.listdir(text_features_dir)):\n","    if file_name.endswith('.npy'):\n","        file_path = os.path.join(text_features_dir, file_name)\n","        text_features = np.load(file_path)\n","        text_feature_list.append(text_features)\n","        text_filenames.append(file_name)\n","\n","# Ensure the video and text features are of the same data type\n","video_features = torch.tensor(np.stack(video_feature_list), dtype=torch.float32)\n","text_features = torch.tensor(np.stack(text_feature_list), dtype=torch.float32)"],"metadata":{"id":"nZyv44Yj3Jao","executionInfo":{"status":"ok","timestamp":1730813360648,"user_tz":-330,"elapsed":9913,"user":{"displayName":"NNAR-CAPSTONE","userId":"01323149097215678909"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Configuring and Running Model"],"metadata":{"id":"oKZgf0hP3U7j"}},{"cell_type":"code","source":["video_dim = video_features.shape[2] if len(video_features.shape) == 3 else video_features.shape[1]\n","text_dim = text_features.shape[2] if len(text_features.shape) == 3 else text_features.shape[1]\n","fusion_dim = 512\n","\n","fusion_model = MultimodalFusion(video_dim, text_dim, fusion_dim)"],"metadata":{"id":"J6VDq3o43VkO","executionInfo":{"status":"ok","timestamp":1730813367005,"user_tz":-330,"elapsed":1016,"user":{"displayName":"NNAR-CAPSTONE","userId":"01323149097215678909"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Saving Fused Features"],"metadata":{"id":"_nkM4mpz3ZB2"}},{"cell_type":"code","source":["output_fused_dir = '/content/drive/MyDrive/cbp_fused'\n","os.makedirs(output_fused_dir, exist_ok=True)\n","\n","for i, video_filename in enumerate(video_filenames):\n","    matching_text_filename = find_matching_text_filename(video_filename, text_filenames)\n","    if matching_text_filename:\n","        video_feat = video_features[i].unsqueeze(0).float()\n","        text_feat_index = text_filenames.index(matching_text_filename)\n","        text_feat = text_features[text_feat_index].unsqueeze(0).float()\n","\n","        fused_feat = fusion_model(video_feat, text_feat)\n","        fused_feat = fused_feat.squeeze(0).detach().cpu().numpy()\n","        #print(fused_feat)\n","\n","        output_filename = f\"{video_filename.split('_embeddings.npy')[0]}_fused.npy\"\n","        output_path = os.path.join(output_fused_dir, output_filename)\n","        np.save(output_path, fused_feat)\n","\n","print(\"Fused features saved successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_FlOIo_3gGP","executionInfo":{"status":"ok","timestamp":1730813375484,"user_tz":-330,"elapsed":4576,"user":{"displayName":"NNAR-CAPSTONE","userId":"01323149097215678909"}},"outputId":"ac9d7c0a-3aac-4718-c939-03139e3b5dd6"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Fused features saved successfully.\n"]}]}]}